# -*- coding: utf-8 -*-
"""4.1-VIT-todos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dTNvcboa-RRZXMWR0mB6tBO3qWdoR8E3

# Reconhecimento Multirrótulo de Acessórios, Gênero e Cores de Vestuário em Ambientes Dinâmicos

Bacharelado em Ciência da Computação / PUCPR

2025

## Imports e dataset
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
import tensorflow_datasets as tfds
import os, csv
from collections import Counter
import random
from PIL import Image, ImageOps
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from sklearn.metrics import confusion_matrix

# DataSet: https://pucpredu-my.sharepoint.com/personal/rayson_santos_pucpr_br/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Frayson%5Fsantos%5Fpucpr%5Fbr%2FDocuments%2FPAR2025%2Ezip&parent=%2Fpersonal%2Frayson%5Fsantos%5Fpucpr%5Fbr%2FDocuments&ga=1

# Baixando o repositório como um arquivo ZIP

os.system("wget https://github.com/MatheusKozak/Clothing-Detection-Challenge/archive/refs/heads/main.zip -O Clothing-Detection-Challenge.zip")

# Descompacta apenas as pastas training_set e validation_set
# Tempo normal para baixar e descompactar é de 3/4 min
os.system("unzip -j Clothing-Detection-Challenge.zip 'Clothing-Detection-Challenge-main/training_set/*' -d training_set")
os.system("unzip -j Clothing-Detection-Challenge.zip 'Clothing-Detection-Challenge-main/validation_set/*' -d validation_set")

"""## Padronizando os tamanhos da imagem"""

IMAGE_SIZE = 224
SEED = 42
SRC_DIR = "training_set"
OUT_TRAIN = f"training_set_resized_{IMAGE_SIZE}"
OUT_TEST  = f"teste_set_resized_{IMAGE_SIZE}"
VALID_EXT = (".jpg", ".jpeg", ".png")

class PadToSquare:
    def __call__(self, img: Image.Image) -> Image.Image:
        w, h = img.size
        if w == h:
            return img
        side = max(w, h)
        dw = (side - w) // 2
        dh = (side - h) // 2
        padding = (dw, dh, side - w - dw, side - h - dh)
        return ImageOps.expand(img, padding, fill=0)

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def save_processed(img_path: str, dst_root: str, image_size: int, padder: PadToSquare):
    ensure_dir(dst_root)
    base = os.path.splitext(os.path.basename(img_path))[0]
    ext = ".jpg"
    out_path = os.path.join(dst_root, base + ext)
    i = 1
    while os.path.exists(out_path):
        out_path = os.path.join(dst_root, f"{base}_{i}{ext}")
        i += 1
    img = Image.open(img_path).convert("RGB")
    img = padder(img)
    img = img.resize((image_size, image_size), Image.BILINEAR)
    img.save(out_path, quality=95)
    return out_path

def preview(paths, k, title):
    if not paths:
        print(f"Sem imagens para preview: {title}")
        return
    sample = random.sample(paths, min(k, len(paths)))
    cols = min(4, k)
    rows = (len(sample) + cols - 1) // cols
    plt.figure(figsize=(4*cols, 4*rows))
    for i, p in enumerate(sample):
        plt.subplot(rows, cols, i+1)
        plt.imshow(Image.open(p))
        plt.title(os.path.basename(p), fontsize=8)
        plt.axis("off")
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

def process_like_old_snippet():
    random.seed(SEED)
    padder = PadToSquare()

    image_files = [
        f for f in os.listdir(SRC_DIR)
        if os.path.isfile(os.path.join(SRC_DIR, f)) and f.lower().endswith(VALID_EXT)
    ]
    total_files = len(image_files)
    if total_files == 0:
        print("Sem imagens na raiz de 'training_set'.")
        return

    random.shuffle(image_files)

    # 9% do total

    sub_set_size = 1
    subset_len = int(0.9 * total_files * sub_set_size)
    sub_set = image_files[:subset_len]

    # Split 90/10 desntro desse subconjunto
    total_files_small = len(sub_set)
    train_len = int(0.9 * total_files_small)
    train_files = sub_set[:train_len]
    test_files  = sub_set[train_len:]

    ensure_dir(OUT_TRAIN); ensure_dir(OUT_TEST)
    out_train_paths, out_test_paths = [], []

    for fname in train_files:
        src_path = os.path.join(SRC_DIR, fname)
        try:
            out_train_paths.append(save_processed(src_path, OUT_TRAIN, IMAGE_SIZE, padder))
        except Exception as e:
            print("Erro (train):", src_path, e)

    for fname in test_files:
        src_path = os.path.join(SRC_DIR, fname)
        try:
            out_test_paths.append(save_processed(src_path, OUT_TEST, IMAGE_SIZE, padder))
        except Exception as e:
            print("Erro (test):", src_path, e)

    print(f"Total de imagens (origem): {total_files}")
    print(f"Subconjunto usado (≈9%):  {total_files_small}")
    print(f"Treino (90% do subset):   {len(out_train_paths)} -> {OUT_TRAIN}")
    print(f"Teste  (10% do subset):   {len(out_test_paths)}  -> {OUT_TEST}")

    preview(out_train_paths, 4, "Preview - 4 aleatórias (Treino)")
    preview(out_test_paths,  4, "Preview - 4 aleatórias (Teste)")


process_like_old_snippet()

"""## Deep learning"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TRAIN_DIR = OUT_TRAIN
TEST_DIR  = OUT_TEST
TRAIN_TXT = "training_set/training_set.txt"
TEST_TXT  = "training_set/training_set.txt"
BATCH_SIZE = 128
EPOCHS = 40
LR = 1e-4
TOP_COL_IDX = 1
BOTTOM_COL_IDX = 2
GENDER_COL_IDX = 3
BAG_COL_IDX = 4
HAT_COL_IDX = 5

"""## Deep learning - VIT

### Com todos os atributos juntos
"""

print(f"Device: {DEVICE}")

def load_all_labels(txt_path):
    """Carrega todos os 5 atributos de uma vez"""
    mapping = {}
    with open(txt_path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            if not row or len(row) < 6:
                continue
            fname = row[0].strip()

            top_color = int(row[TOP_COL_IDX].strip()) - 1      # 0-10
            bottom_color = int(row[BOTTOM_COL_IDX].strip()) - 1 # 0-10
            gender = int(row[GENDER_COL_IDX].strip())           # 0-1
            bag = int(row[BAG_COL_IDX].strip())                 # 0-1
            hat = int(row[HAT_COL_IDX].strip())                 # 0-1

            mapping[fname] = {
                'top_color': top_color,
                'bottom_color': bottom_color,
                'gender': gender,
                'bag': bag,
                'hat': hat
            }
    return mapping

train_labels = load_all_labels(TRAIN_TXT)
test_labels  = load_all_labels(TEST_TXT)

def list_labeled_files(img_dir, labels_dict):
    files = [f for f in os.listdir(img_dir) if f.lower().endswith((".jpg",".jpeg",".png"))]
    keep  = [f for f in files if f in labels_dict]
    return keep

train_files = list_labeled_files(TRAIN_DIR, train_labels)
test_files  = list_labeled_files(TEST_DIR,  test_labels)

if len(train_files) == 0 or len(test_files) == 0:
    raise RuntimeError("Algum split ficou vazio após cruzar arquivos com rótulos.")

print(f"Train: {len(train_files)} | Test: {len(test_files)}")

train_tfms = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406],
                         std=[0.229,0.224,0.225]),
])

test_tfms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406],
                         std=[0.229,0.224,0.225]),
])

class MultiTaskDataset(Dataset):
    def __init__(self, file_list, labels_dict, root_dir, transform=None):
        self.file_list = file_list
        self.labels_dict = labels_dict
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        fname = self.file_list[idx]
        path = os.path.join(self.root_dir, fname)
        img = Image.open(path).convert("RGB")

        if self.transform:
            img = self.transform(img)

        labels = self.labels_dict[fname]

        return img, {
            'top_color': torch.tensor(labels['top_color'], dtype=torch.long),
            'bottom_color': torch.tensor(labels['bottom_color'], dtype=torch.long),
            'gender': torch.tensor(labels['gender'], dtype=torch.long),
            'bag': torch.tensor(labels['bag'], dtype=torch.long),
            'hat': torch.tensor(labels['hat'], dtype=torch.long)
        }

train_ds = MultiTaskDataset(train_files, train_labels, TRAIN_DIR, transform=train_tfms)
test_ds  = MultiTaskDataset(test_files,  test_labels,  TEST_DIR,  transform=test_tfms)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

# Modelo Multi-Task com Vision Transformer
class ViTMultiTaskV2(nn.Module):
    def __init__(self):
        super(ViTMultiTaskV2, self).__init__()

        # Backbone compartilhado (Vision Transformer)
        vit = models.vision_transformer.vit_b_16(weights='IMAGENET1K_V1')
        self.backbone = vit

        # ViT já faz forward completo e retorna features de 1000 dims
        # 5 cabeças de classificação (uma para cada atributo)
        self.fc_top_color = nn.Linear(1000, 11)     # 11 cores
        self.fc_bottom_color = nn.Linear(1000, 11)  # 11 cores
        self.fc_gender = nn.Linear(1000, 2)         # 2 classes
        self.fc_bag = nn.Linear(1000, 2)            # 2 classes
        self.fc_hat = nn.Linear(1000, 2)            # 2 classes

    def forward(self, x):
        # Features do ViT
        features = self.backbone(x)  # ViT já faz forward completo

        # Predições de cada atributo
        out_top_color = self.fc_top_color(features)
        out_bottom_color = self.fc_bottom_color(features)
        out_gender = self.fc_gender(features)
        out_bag = self.fc_bag(features)
        out_hat = self.fc_hat(features)

        return {
            'top_color': out_top_color,
            'bottom_color': out_bottom_color,
            'gender': out_gender,
            'bag': out_bag,
            'hat': out_hat
        }

model = ViTMultiTaskV2().to(DEVICE)

# Loss para cada atributo
criterion = nn.CrossEntropyLoss()

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)

def run_epoch(model, loader, train_mode=True):
    model.train() if train_mode else model.eval()

    total_loss = 0.0
    correct = {'top_color': 0, 'bottom_color': 0, 'gender': 0, 'bag': 0, 'hat': 0}
    total = 0

    with torch.set_grad_enabled(train_mode):
        for imgs, labels in loader:
            imgs = imgs.to(DEVICE)

            # Move todos os labels para device
            labels_device = {k: v.to(DEVICE) for k, v in labels.items()}

            if train_mode:
                optimizer.zero_grad()

            # Forward pass
            outputs = model(imgs)

            # Calcular loss de cada atributo
            loss_top = criterion(outputs['top_color'], labels_device['top_color'])
            loss_bottom = criterion(outputs['bottom_color'], labels_device['bottom_color'])
            loss_gender = criterion(outputs['gender'], labels_device['gender'])
            loss_bag = criterion(outputs['bag'], labels_device['bag'])
            loss_hat = criterion(outputs['hat'], labels_device['hat'])

            # Loss total (soma de todas)
            loss = loss_top + loss_bottom + loss_gender + loss_bag + loss_hat

            if train_mode:
                loss.backward()
                optimizer.step()

            total_loss += loss.item() * imgs.size(0)

            # Calcular acurácia de cada atributo
            for attr in ['top_color', 'bottom_color', 'gender', 'bag', 'hat']:
                preds = outputs[attr].argmax(dim=1)
                correct[attr] += (preds == labels_device[attr]).sum().item()

            total += labels_device['gender'].size(0)

    # Acurácias individuais
    acc = {k: correct[k] / total for k in correct.keys()}
    avg_acc = sum(acc.values()) / len(acc)

    return total_loss/total, acc, avg_acc

# Treinamento
best_avg_acc = 0.0

print("\nTreinando Vision Transformer Multi-Task...")

for epoch in range(1, EPOCHS+1):
    tr_loss, tr_acc, tr_avg = run_epoch(model, train_loader, train_mode=True)
    te_loss, te_acc, te_avg = run_epoch(model, test_loader,  train_mode=False)

    print(f"\nEpoch {epoch:02d}/{EPOCHS}")
    print(f"Train Avg: {tr_avg*100:.1f}% | Test Avg: {te_avg*100:.1f}%")
    print(f"  Top Color:    Train {tr_acc['top_color']*100:.1f}% | Test {te_acc['top_color']*100:.1f}%")
    print(f"  Bottom Color: Train {tr_acc['bottom_color']*100:.1f}% | Test {te_acc['bottom_color']*100:.1f}%")
    print(f"  Gender:       Train {tr_acc['gender']*100:.1f}% | Test {te_acc['gender']*100:.1f}%")
    print(f"  Bag:          Train {tr_acc['bag']*100:.1f}% | Test {te_acc['bag']*100:.1f}%")
    print(f"  Hat:          Train {tr_acc['hat']*100:.1f}% | Test {te_acc['hat']*100:.1f}%")

    # Scheduler baseado na acurácia média
    scheduler.step(te_avg)

    # Salvar melhor modelo
    if te_avg > best_avg_acc:
        best_avg_acc = te_avg
        torch.save(model.state_dict(), 'best_vit_multitask_v2.pth')
        print(f"✓ Melhor modelo salvo! Avg Acc: {best_avg_acc*100:.2f}%")

print(f"\n{'='*60}")
print(f"Melhor acurácia média: {best_avg_acc*100:.1f}%")
print(f"{'='*60}")

# Avaliação final com melhor modelo
model.load_state_dict(torch.load('best_vit_multitask_v2.pth'))
_, final_acc, final_avg = run_epoch(model, test_loader, train_mode=False)

print("\nAcurácias finais no test set:")
print(f"  Top Color:    {final_acc['top_color']*100:.1f}%")
print(f"  Bottom Color: {final_acc['bottom_color']*100:.1f}%")
print(f"  Gender:       {final_acc['gender']*100:.1f}%")
print(f"  Bag:          {final_acc['bag']*100:.1f}%")
print(f"  Hat:          {final_acc['hat']*100:.1f}%")
print(f"  Média:        {final_avg*100:.1f}%")